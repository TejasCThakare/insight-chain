training:
  # Output
  output_dir: "models/reasoning_agent"
  
  # Training
  num_train_epochs: 2
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  per_device_eval_batch_size: 1
  
  # Optimization
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 50
  max_grad_norm: 1.0
  optim: "adamw_torch"
  
  # Precision & Memory
  fp16: true
  gradient_checkpointing: true
  
  # Logging & Saving
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  evaluation_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 2
  load_best_model_at_end: false
  metric_for_best_model: "loss"
  report_to: "none"
  
  # Data
  max_seq_length: 1024
  dataloader_num_workers: 0
