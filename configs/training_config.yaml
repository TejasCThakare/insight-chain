training:
  # Output
  output_dir: "models/reasoning_agent"

  # Training
  num_train_epochs: 2
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 4
  per_device_eval_batch_size: 1

  # Optimization
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_steps: 50
  max_grad_norm: 1.0
  optim: "paged_adamw_8bit"

  # Precision & Memory
  fp16: true
  gradient_checkpointing: true

  # Logging & Saving
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 2
  load_best_model_at_end: true
  report_to: "none"

  # Data
  max_seq_length: 1024
  dataloader_num_workers: 0
